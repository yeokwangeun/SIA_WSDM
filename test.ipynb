{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_data, TrainDataset, EvalDataset\n",
    "from dataloader import DataLoaderHandler\n",
    "from model import SetTransformer\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.modules import PreNorm, Attention, FeedForward\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args([])\n",
    "args.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "args.dataset = \"amazon_beauty\"\n",
    "args.maxlen = 50\n",
    "args.batch_size = 64\n",
    "args.content = [\"image\", \"desc\"]\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "args.latent_dim = 128\n",
    "\n",
    "args.item_num_outputs = 8\n",
    "args.item_num_heads = 8\n",
    "args.item_num_latents = 8\n",
    "args.item_dim_hidden = 64\n",
    "\n",
    "args.attn_num_iters = 5\n",
    "args.attn_self_per_cross = 2\n",
    "args.attn_dropout = 0.0\n",
    "args.attn_num_heads = 8\n",
    "args.attn_dim_head = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = \"./dataset/raw/\"\n",
    "processed_dir = \"./dataset/processed/\"\n",
    "\n",
    "inter, item_feats, pop = load_data(args, raw_dir, processed_dir, logger)\n",
    "train_dataset = TrainDataset(inter, item_feats, args, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoaderHandler(\"train\", train_dataset, args, logger)\n",
    "train_loader = train_loader.get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "item_feat_dims = [tuple(feat.values())[0].shape[0] for feat in item_feats]\n",
    "n_items = len(pop)\n",
    "\n",
    "# LAYERS\n",
    "set_transformers = [\n",
    "    SetTransformer(\n",
    "        dim_input=feat_dim,\n",
    "        num_outputs=args.item_num_outputs,\n",
    "        dim_output=args.latent_dim,\n",
    "        num_inds=args.item_num_latents,\n",
    "        dim_hidden=args.item_dim_hidden,\n",
    "        num_heads=args.item_num_heads,\n",
    "        ln=True,\n",
    "    ) for feat_dim in item_feat_dims\n",
    "]\n",
    "id_embedding = nn.Embedding(\n",
    "    num_embeddings=(n_items + 1),\n",
    "    embedding_dim=args.latent_dim,\n",
    "    device=args.device,\n",
    "    padding_idx=0\n",
    ")\n",
    "pos_embedding = nn.Embedding(\n",
    "    num_embeddings=(args.maxlen + 1),\n",
    "    embedding_dim=args.latent_dim,\n",
    "    device=args.device,\n",
    "    padding_idx=0\n",
    ")\n",
    "# id_set_transformer = SetTransformer(\n",
    "#     dim_input=args.emb_size,\n",
    "#     num_outputs=16,\n",
    "#     dim_output=args.emb_size,\n",
    "#     num_inds=16,\n",
    "#     dim_hidden=args.st_dim_hidden,\n",
    "#     num_heads=args.st_num_heads,\n",
    "#     ln=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT\n",
    "# seq_list: (B, N)\n",
    "# next_item_list: (B,)\n",
    "# item_feat_list: (B, N, d)\n",
    "seq_list, pos_list, next_item_list, *item_feat_lists = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_emb = id_embedding(seq_list)\n",
    "pos_emb = pos_embedding(pos_list)\n",
    "latents = id_emb + pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feat = []\n",
    "for set_transformer, item_feat_list in zip(set_transformers, item_feat_lists):\n",
    "    out = [set_transformer(feat.unsqueeze(0)) for feat in item_feat_list]\n",
    "    out = torch.cat(out)\n",
    "    item_feat.append(out)\n",
    "item_feat = torch.cat(item_feat, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_latent = repeat(pos_list, 'b n -> b n d', d=args.latent_dim).float()\n",
    "mask_items = torch.ones(item_feat.shape)\n",
    "mask_cross_attn = einsum(\"b i d, b j d -> b i j\", mask_latent, mask_items) > 0\n",
    "mask_self_attn = einsum(\"b i d, b j d -> b i j\", mask_latent, mask_latent) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn_beg = Attention(\n",
    "    query_dim=args.latent_dim,\n",
    "    heads=args.attn_num_heads,\n",
    "    dim_head=args.attn_dim_head,\n",
    "    dropout=args.attn_dropout\n",
    ")\n",
    "\n",
    "self_attn = Attention(\n",
    "    query_dim=args.latent_dim,\n",
    "    heads=args.attn_num_heads,\n",
    "    dim_head=args.attn_dim_head,\n",
    "    dropout=args.attn_dropout\n",
    ")\n",
    "\n",
    "cross_attn = PreNorm(\n",
    "    args.latent_dim, \n",
    "    Attention(\n",
    "        query_dim=args.latent_dim,\n",
    "        context_dim=args.latent_dim,\n",
    "        heads=args.attn_num_heads,\n",
    "        dim_head=args.attn_dim_head,\n",
    "        dropout=args.attn_dropout\n",
    "    ), \n",
    "    context_dim=args.latent_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = self_attn(latents, mask=mask_self_attn)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() got an unexpected keyword argument 'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[39m=\u001b[39m cross_attn(latents, context\u001b[39m=\u001b[39;49mitem_feat, mask\u001b[39m=\u001b[39;49mmask_cross_attn)\n\u001b[1;32m      2\u001b[0m out\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() got an unexpected keyword argument 'context'"
     ]
    }
   ],
   "source": [
    "out = cross_attn(latents, context=item_feat, mask=mask_cross_attn)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = embedded\n",
    "k = item_feat_concat\n",
    "v = item_feat_concat\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_q = q.shape[-1]\n",
    "dim_k = k.shape[-1]\n",
    "dim_v = v.shape[-1]\n",
    "num_heads = 8\n",
    "dim_head = 64\n",
    "inner_dim = dim_head * num_heads\n",
    "scale = dim_head ** -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_q = nn.Linear(dim_q, inner_dim, bias=False)\n",
    "fc_k = nn.Linear(dim_k, inner_dim, bias=False)\n",
    "fc_v = nn.Linear(dim_v, inner_dim, bias=False)\n",
    "\n",
    "dropout = nn.Dropout(0.2)\n",
    "to_out = nn.Linear(inner_dim, dim_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = fc_q(q)\n",
    "k = fc_k(k)\n",
    "v = fc_v(v)\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> (b h) n d\", h=num_heads), (q, k, v))\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = einsum(\"b i d, b j d -> b i j\", q, k) * scale\n",
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_q = repeat(seq_list, 'b n -> b n d', d=args.emb_size).float()\n",
    "mask_kv = torch.ones(item_feat_concat.shape)\n",
    "mask = einsum(\"b i d, b j d -> b i j\", mask_q, mask_kv) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = repeat(mask, \"b i j -> (b h) i j\", h=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_neg_value = -torch.finfo(sim.dtype).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.masked_fill_(~mask, max_neg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = sim.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = dropout(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = einsum(\"b i j, b j d -> b i d\", attn, v)\n",
    "out = rearrange(out, \"(b h) n d -> b n (h d)\", h=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_out(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_list, next_item_list, *item_feat_lists in tqdm(train_loader):\n",
    "    item_out = []\n",
    "    for set_transformer, item_feat_list in zip(set_transformers, item_feat_lists):\n",
    "        out = [set_transformer(item_feat.unsqueeze(0)) for item_feat in item_feat_list]\n",
    "        out = torch.cat(out)\n",
    "        item_out.append(out)\n",
    "    item_out = torch.cat(item_out, dim=1)\n",
    "\n",
    "    seq_out = [id_set_transformer(embedding(seq).unsqueeze(0)) for seq in seq_list]\n",
    "    seq_out = torch.cat(seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = len(pop)\n",
    "embedding = nn.Embedding(\n",
    "    num_embeddings=(n_items + 1),\n",
    "    embedding_dim=args.emb_size,\n",
    "    padding_idx=0,\n",
    "    device=args.device\n",
    ")\n",
    "\n",
    "embedded = embedding(seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = (seq_list == 0)\n",
    "attn_mask = (padded.unsqueeze(2) | padded.unsqueeze(1)).repeat(args.ia_num_heads, 1, 1)\n",
    "attn_output, _ = ma(query=embedded, key=embedded, value=embedded, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feat_mask = torch.full(item_feat_concat.shape, False)[:, :, 0]\n",
    "attn_mask = padded.unsqueeze(2) | item_feat_mask.unsqueeze(1)\n",
    "attn_mask = attn_mask.repeat(args.ia_num_heads, 1, 1)\n",
    "attn_output, _ = ma(query=attn_output, key=item_feat_concat, value=item_feat_concat, attn_mask=attn_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
